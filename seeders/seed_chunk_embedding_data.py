import json
import random
import datetime

def run(conn):
    cursor = conn.cursor()
    embedding_models = ['openai-text-embedding-ada-002', 'cohere-embed', 'azure-embeddings', 'local-sentence-transformer']
    versions = ['v1', 'v2', '2024.02', '2025.01']
    strategies = ['re_chunk', 'normalize', 'merge', 'split']
    
    for i in range(50):
        chunk_id = f"CHUNK_{2000 + i}"
        doc_id = f"DOC_{1000 + random.randint(0, 49)}"
        embedding_model = random.choice(embedding_models)
        embedding_version = random.choice(versions)
        quality_score = round(random.uniform(0.6, 1.0), 2)
        reindex_count = random.randint(0, 4)
        healing_suggestions = json.dumps({
            "strategy": random.choice(strategies),
            "reason": "Autogenerated suggestion for improving embedding chunk quality.",
            "suggested_params": {
                "new_chunk_size": random.choice([512, 768, 1024]),
                "overlap": random.choice([50, 80, 120])
            }
        })
        created_at = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        last_healed = (datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 30))).strftime("%Y-%m-%d %H:%M:%S")
        
        cursor.execute('''
            INSERT OR IGNORE INTO chunk_embedding_data (
                chunk_id, doc_id, embedding_model, embedding_version,
                quality_score, reindex_count, healing_suggestions,
                created_at, last_healed
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            chunk_id, doc_id, embedding_model, embedding_version,
            quality_score, reindex_count, healing_suggestions,
            created_at, last_healed
        ))
    conn.commit()
